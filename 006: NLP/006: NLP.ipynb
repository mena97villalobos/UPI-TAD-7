{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPeQYY0R0JZOLG6VStWiqC1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Tokenization\n","Tenemos 2 ejemplos\n","\n","*   Utilizando el paquete spacy\n","*   Utilizando el paquete nltk\n","\n"],"metadata":{"id":"0Tc42UWIO9Cn"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5HeQCQbjDr-j","executionInfo":{"status":"ok","timestamp":1719536478186,"user_tz":360,"elapsed":1615,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"8f7def5e-a483-49b0-f4ad-d6b2f3c9efe5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Tokenization',\n"," 'is',\n"," 'the',\n"," 'process',\n"," 'of',\n"," 'breaking',\n"," 'down',\n"," 'text',\n"," 'into',\n"," ' ',\n"," 'tokens',\n"," '.']"]},"metadata":{},"execution_count":2}],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","doc = nlp(\"Tokenization is the process of breaking down text into  tokens.\")\n","tokens = [token.text for token in doc]\n","tokens"]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","nltk.download('punkt')\n","\n","text = \"Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. Challenges in natural language processing frequently involve natural language understanding, natural language generation (frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\"\n","\n","print(sent_tokenize(text))\n","print(word_tokenize(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7KIdeHwUPNfo","executionInfo":{"status":"ok","timestamp":1719517675593,"user_tz":360,"elapsed":1603,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"7c7b5ece-e27c-4800-9721-017abfdc9630"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["['Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.', 'Challenges in natural language processing frequently involve natural language understanding, natural language generation (frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.']\n","['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', '(', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.']\n"]}]},{"cell_type":"markdown","source":["# Stopwords\n"],"metadata":{"id":"gf-1JjxZUEwv"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","\n","nltk.download('stopwords')\n","\n","stopwords_en = stopwords.words('english')\n","stopwords_es = stopwords.words('spanish')\n","print(stopwords_en)\n","print(stopwords_es)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X42oDxNQUGy4","executionInfo":{"status":"ok","timestamp":1719519296471,"user_tz":360,"elapsed":668,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"764b9b9a-1a03-4fbb-ed3e-c3b44b612721"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["text = \"Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. Challenges in natural language processing frequently involve natural language understanding, natural language generation (frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\"\n","clean_text = [word for word in text.split(\" \") if word not in stopwords_en]\n","\n","print(text.split(\" \"))\n","print(clean_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DdhX8dR-VoqH","executionInfo":{"status":"ok","timestamp":1719519385478,"user_tz":360,"elapsed":303,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"20dc5c73-5abf-47ba-b8cd-bdb637eb476e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Natural', 'language', 'processing', '(NLP)', 'is', 'a', 'field', 'of', 'computer', 'science,', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(natural)', 'languages,', 'and,', 'in', 'particular,', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding,', 'natural', 'language', 'generation', '(frequently', 'from', 'formal,', 'machine-readable', 'logical', 'forms),', 'connecting', 'language', 'and', 'machine', 'perception,', 'managing', 'human-computer', 'dialog', 'systems,', 'or', 'some', 'combination', 'thereof.']\n","['Natural', 'language', 'processing', '(NLP)', 'field', 'computer', 'science,', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'interactions', 'computers', 'human', '(natural)', 'languages,', 'and,', 'particular,', 'concerned', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora.', 'Challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding,', 'natural', 'language', 'generation', '(frequently', 'formal,', 'machine-readable', 'logical', 'forms),', 'connecting', 'language', 'machine', 'perception,', 'managing', 'human-computer', 'dialog', 'systems,', 'combination', 'thereof.']\n"]}]},{"cell_type":"markdown","source":["# Stemming"],"metadata":{"id":"28NkBP9ybIAO"}},{"cell_type":"code","source":["import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","stemmer = PorterStemmer()\n","sentence = \"The quick brown foxes are jumping over the lazy dogs\"\n","\n","words = word_tokenize(sentence)\n","stemmed_words = [stemmer.stem(word) for word in words]\n","\n","print(\"Original Sentence:\", sentence.split(\" \"))\n","print(\"Stemmed Words:\", stemmed_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7jvAjuwbJTR","executionInfo":{"status":"ok","timestamp":1719520799179,"user_tz":360,"elapsed":328,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"a322c0f0-bffc-4244-9dee-c15b8f757666"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Sentence: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs']\n","Stemmed Words: ['the', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazi', 'dog']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["# Lemmatization"],"metadata":{"id":"PoLCWIW-bgeX"}},{"cell_type":"code","source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","sentence = \"The quick brown foxes are jumping over the lazy dogs\"\n","words = word_tokenize(sentence)\n","lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n","\n","# Print the original sentence and the lemmatized words\n","print(\"Original Sentence:\", sentence.split(\" \"))\n","print(\"Lemmatized Words:\", lemmatized_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L27DEl1lbh0z","executionInfo":{"status":"ok","timestamp":1719520962974,"user_tz":360,"elapsed":693,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"e53ef054-71d5-4cef-886e-b736670de3fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Sentence: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs']\n","Lemmatized Words: ['The', 'quick', 'brown', 'fox', 'be', 'jump', 'over', 'the', 'lazy', 'dog']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["# Normalization"],"metadata":{"id":"5PV85U3vc_KV"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import string\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Example sentence\n","sentence = \"The quick brown foxes are jumping over the lazy dogs. Isn't this exciting?\"\n","\n","# Step 1: Lowercasing\n","normalized_sentence = sentence.lower()\n","\n","# Step 2: Tokenization\n","tokens = word_tokenize(normalized_sentence)\n","\n","# Step 3: Removing stopwords and punctuation\n","punctuation = set(string.punctuation)\n","filtered_tokens = [token for token in tokens if token not in punctuation]\n","\n","# Print the original sentence and the normalized tokens\n","print(\"Original Sentence:\", sentence.split(\" \"))\n","print(\"Normalized Tokens:\", filtered_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JFFm3IKRdAaf","executionInfo":{"status":"ok","timestamp":1719521297453,"user_tz":360,"elapsed":359,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"c1bdb2b6-9a10-4ece-a4b3-a60b7d13ca6c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Sentence: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs.', \"Isn't\", 'this', 'exciting?']\n","Normalized Tokens: ['the', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', 'is', \"n't\", 'this', 'exciting']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["import string\n","string.punctuation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"IqCk_jdtaS8i","executionInfo":{"status":"ok","timestamp":1719537337696,"user_tz":360,"elapsed":270,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"9b48fd5a-6e47-4c54-b98f-943b6d264053"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["# Noise & Outliers"],"metadata":{"id":"3nVrs8tueDyO"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Example corpus (a small collection of sentences)\n","corpus = [\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"Python is a versatile programming language used in data science.\",\n","    \"Python is a versatile programming language used widely in data science.\",\n","    \"Today's weather forecast predicts rain in the evening.\",\n","    \"Today's weather forecast predicts heavy rain in the evening.\",\n","    \"xxyyzzaabb is a rare genetic disorder affecting 1 in 100,000 people.\",\n","    \"is a rare disorder affecting 1 in 100,000 people.\"\n","]\n","\n","# Tokenize and normalize the corpus\n","tokenized_words = []\n","stop_words = set(stopwords.words('english'))\n","\n","for sentence in corpus:\n","    words = word_tokenize(sentence.lower())\n","    tokenized_words.extend(words)\n","\n","# Count frequencies of each word\n","word_freq = Counter(tokenized_words)\n","\n","# Identify outliers (rare words)\n","outliers = [word for word, freq in word_freq.items() if freq == 1]\n","\n","print(\"Original Corpus:\\n\", \"\\n\".join(corpus))\n","print(\"\\nOutliers (Rare Words):\", outliers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-LfL6azdeFQe","executionInfo":{"status":"ok","timestamp":1719521617793,"user_tz":360,"elapsed":330,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"916e7b91-f24e-4d8d-a9e8-b1d48cfe9224"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Corpus:\n"," The quick brown fox jumps over the lazy dog.\n","The quick brown fox jumps over the lazy dog.\n","Python is a versatile programming language used in data science.\n","Python is a versatile programming language used widely in data science.\n","Today's weather forecast predicts rain in the evening.\n","Today's weather forecast predicts heavy rain in the evening.\n","xxyyzzaabb is a rare genetic disorder affecting 1 in 100,000 people.\n","is a rare disorder affecting 1 in 100,000 people.\n","\n","Outliers (Rare Words): ['widely', 'heavy', 'xxyyzzaabb', 'genetic']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["import string\n","\n","# Example sentence with noise\n","noisy_sentence = \"The quick brown fox jumps over the lazy dog!!!\"\n","\n","# Remove punctuation\n","cleaned_sentence = noisy_sentence.translate(str.maketrans('', '', string.punctuation))\n","\n","print(\"Noisy Sentence:\", noisy_sentence)\n","print(\"Cleaned Sentence:\", cleaned_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wCQ3G0lfAKx","executionInfo":{"status":"ok","timestamp":1719521775518,"user_tz":360,"elapsed":364,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"dc0c7892-a6e2-4997-d416-cc80a44043fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Noisy Sentence: The quick brown fox jumps over the lazy dog!!!\n","Cleaned Sentence: The quick brown fox jumps over the lazy dog\n"]}]},{"cell_type":"markdown","source":["# Vectorization"],"metadata":{"id":"g8XkeLIWg1fG"}},{"cell_type":"markdown","source":["## TF-IDF"],"metadata":{"id":"J5p0Ona8hFDd"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Example corpus\n","corpus = [\n","    \"This is the first document.\",\n","    \"This document is the second document.\",\n","    \"And this is the third one.\",\n","    \"Is this the first document?\",\n","]\n","\n","# Initialize TF-IDF Vectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Fit and transform the corpus to TF-IDF vectors\n","tfidf_matrix = vectorizer.fit_transform(corpus)\n","\n","# Print the feature names (words) in the TF-IDF matrix\n","print(\"Feature names:\", vectorizer.get_feature_names_out())\n","\n","# Print the TF-IDF matrix (sparse format)\n","print(\"TF-IDF Matrix:\")\n","print(tfidf_matrix.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_G-RQrzug24s","executionInfo":{"status":"ok","timestamp":1719537910973,"user_tz":360,"elapsed":1924,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"6fd174ee-eedf-44d9-d946-8622e37ab0dc"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n","TF-IDF Matrix:\n","[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]\n"," [0.         0.6876236  0.         0.28108867 0.         0.53864762\n","  0.28108867 0.         0.28108867]\n"," [0.51184851 0.         0.         0.26710379 0.51184851 0.\n","  0.26710379 0.51184851 0.26710379]\n"," [0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]]\n"]}]},{"cell_type":"markdown","source":["## BoW"],"metadata":{"id":"KF_x6i4ihJv-"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Example corpus (a list of documents)\n","corpus = [\n","    \"This is the first document.\",\n","    \"This document is the second document.\",\n","    \"And this is the third one.\",\n","    \"Is this the first document?\",\n","]\n","\n","# Initialize CountVectorizer\n","vectorizer = CountVectorizer()\n","\n","# Fit and transform the corpus to BoW vectors\n","bow_matrix = vectorizer.fit_transform(corpus)\n","\n","# Print feature names (words)\n","print(\"Feature names:\", vectorizer.get_feature_names_out())\n","\n","# Print BoW matrix (sparse format)\n","print(\"BoW Matrix:\")\n","print(bow_matrix.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"scEfLn8IhLCz","executionInfo":{"status":"ok","timestamp":1719537910973,"user_tz":360,"elapsed":2,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"b585597d-b8e3-48ea-b1cb-6dd37a875765"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n","BoW Matrix:\n","[[0 1 1 1 0 0 1 0 1]\n"," [0 2 0 1 0 1 1 0 1]\n"," [1 0 0 1 1 0 1 1 1]\n"," [0 1 1 1 0 0 1 0 1]]\n"]}]},{"cell_type":"markdown","source":["# Manejar datos no balanceados"],"metadata":{"id":"srxRXu19jefr"}},{"cell_type":"code","source":["from imblearn.over_sampling import SMOTE\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","from sklearn.datasets import make_classification\n","\n","# Generate a synthetic imbalanced dataset\n","X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n","                           weights=[0.95, 0.05], random_state=42)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Apply SMOTE to the training data only\n","smote = SMOTE(random_state=42)\n","X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n","\n","# Initialize and train a classifier (e.g., Random Forest)\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train_resampled, y_train_resampled)\n","\n","# Predict on the test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate the model\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_g85k2qLjeLo","executionInfo":{"status":"ok","timestamp":1719523001936,"user_tz":360,"elapsed":866,"user":{"displayName":"Bryan Mena","userId":"04008514182180793384"}},"outputId":"e7ad377c-b56f-4b86-ab99-ab81b9852c2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.98      0.97       189\n","           1       0.57      0.36      0.44        11\n","\n","    accuracy                           0.95       200\n","   macro avg       0.77      0.67      0.71       200\n","weighted avg       0.94      0.95      0.94       200\n","\n"]}]}]}